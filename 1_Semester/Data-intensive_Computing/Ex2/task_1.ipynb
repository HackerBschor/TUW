{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6fbf511-c99e-45a7-ab30-382310425b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_file: String = hdfs:///user/pknees/amazon-reviews/full/reviewscombined.json\n",
       "stopwords_file: String = hdfs:///user/e12132344/stopwords.txt\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Test: hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json \n",
    "// Full: hdfs:///user/pknees/amazon-reviews/full/reviewscombined.json\n",
    "val data_file = \"hdfs:///user/pknees/amazon-reviews/full/reviewscombined.json\"\n",
    "val stopwords_file = \"hdfs:///user/e12132344/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8cc7d4e-9551-4599-a361-402a8502797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `hdfs:///user/e12132344/output_rdd': No such file or directory\n",
      "\n",
      "\n",
      "rm: cannot remove ‘output_rdd.txt’: No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r hdfs:///user/e12132344/output_rdd\n",
    "!rm output_rdd.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a7447-1abd-42cb-b1a5-b8a78100b7c8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abaea04c-5a91-4a13-9079-73325d8c71f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.json4s._\n",
       "import org.json4s.jackson.JsonMethods._\n",
       "import java.util.StringTokenizer\n",
       "import scala.collection.mutable.ListBuffer\n",
       "import scala.collection.mutable.HashSet\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import java.util.StringTokenizer\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import scala.collection.mutable.HashSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159417f-67df-424e-99c6-22d066132c63",
   "metadata": {},
   "source": [
    "## Functions & Parsing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ce91088-899b-47b4-8b22-911b6739a995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stop_words_file: org.apache.spark.rdd.RDD[String] = hdfs:///user/e12132344/stopwords.txt MapPartitionsRDD[399] at textFile at <console>:96\n",
       "stop_words: scala.collection.mutable.HashSet[String] = Set(overall, at, seemed, baby, it, together, very, j, down, b, toward, used, enough, thereby, least, gotten, hopefully, being, relatively, except, certainly, yours, lately, serious, further, something, trying, sure, truly, kitchen, us, away, uses, contain, rd, these, ve, looking, where, necessary, toy, okay, described, nobody, as, appropriate, placed, on, is, hadn, already, a, having, be, movie, beside, different, respectively, just, former, everyone, ask, indicates, novel, entirely, lest, insofar, x, its, nothing, aa, mower, seem, allow, ever, use, corresponding, able, hereupon, several, install..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Parsing Stop Words\n",
    "val stop_words_file = sc.textFile(stopwords_file)\n",
    "val stop_words: HashSet[String] = HashSet()\n",
    "stop_words_file.collect().foreach(v => stop_words += v)\n",
    "\n",
    "// Review Text -> tokens: List[String]\n",
    "def tokenize(text: String): List[String] = {\n",
    "    val tokenizer = new StringTokenizer(text, \" ()[]{}.!?,;:+=-_\\\"'`~#@&*%€$§\\\\/\")\n",
    "    var tokens = new ListBuffer[String]()\n",
    "     while (tokenizer.hasMoreTokens()) {\n",
    "      tokens += tokenizer.nextToken().toLowerCase()\n",
    "     }\n",
    "    tokens.toList\n",
    "}\n",
    "\n",
    "// Get String from Option\n",
    "def option_to_string(x: Option[String]): String = x match {\n",
    "  case Some(s) => s\n",
    "  case None => \"?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "248c5a81-9b9b-461d-b38b-026a28cb0dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews_file: org.apache.spark.rdd.RDD[String] = hdfs:///user/pknees/amazon-reviews/full/reviewscombined.json MapPartitionsRDD[405] at textFile at <console>:107\n",
       "reviews_json: org.apache.spark.rdd.RDD[Map[String,String]] = MapPartitionsRDD[406] at map at <console>:110\n",
       "reviews: org.apache.spark.rdd.RDD[(String, List[String])] = MapPartitionsRDD[407] at map at <console>:111\n",
       "num_docs: Long = 78828876\n",
       "categories_tokens: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[408] at flatMapValues at <console>:117\n",
       "tokens: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[409] at flatMap at <console>:119\n",
       "categories: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[410] at map at <console>:120\n",
       "count_tokens: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[412] at reduceByKey at ..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading file \n",
    "val reviews_file = sc.textFile(file)\n",
    "\n",
    "// Parsing JSON -> extracting \"category\" & \"reaviewText\" (tokenize -> case folding -> stop word filtering)\n",
    "val reviews_json = reviews_file.map(line => parse(line).values.asInstanceOf[Map[String, String]])\n",
    "val reviews = reviews_json.map(j => \n",
    "    (option_to_string(j.get(\"category\")), tokenize(j.get(\"reviewText\").toString()).filter(e => !stop_words(e)))) \n",
    "\n",
    "val num_docs = reviews_json.count()\n",
    "\n",
    "// <category> -> [<term1>, <term2>, ... ] => [<category> -> <term1>, <category> -> <term2>, ...] \n",
    "val categories_tokens = reviews.flatMapValues(v => v)\n",
    "\n",
    "val tokens = reviews.flatMap(j => j._2)\n",
    "val categories = reviews.map(j => j._1)\n",
    "\n",
    "// Count token, category, category&token\n",
    "val count_tokens = tokens.map(v => (v,1)).reduceByKey((a,b) => a+b)\n",
    "val count_category = categories.map(v => (v,1)).reduceByKey((a,b) => a+b)\n",
    "val count_categoriestokens = categories_tokens.map(v => (v,1)).reduceByKey((a,b) => a+b)\n",
    "\n",
    "// Joins count_categories_tokens with count_tokens ON token -> ReMap on category \n",
    "val count_tokens_categoriestokens = count_categories_tokens.map(v => (v._1._2, (v._1._1, v._2))).join(count_tokens)\n",
    "    .map(v => (v._2._1._1, (v._1, v._2._1._2, v._2._2)))\n",
    "\n",
    "/*\n",
    " * Joins count_category with count_tokens_categoriestokens ON category \n",
    " * Calculate Chi^2 Value for every category X term\n",
    " * * ReMap on (category, (token, chi_square))\n",
    " */\n",
    "val chi_square_values = count_tokens_categoriestokens.join(count_category).map(v => {\n",
    "    val category = v._1\n",
    "    val term = v._2._1._1\n",
    "    val count_term_cat = v._2._1._2\n",
    "    val count_term = v._2._1._3\n",
    "    val count_cat = v._2._2\n",
    "    \n",
    "    val a: Double = count_term_cat\n",
    "    val b: Double = count_term - count_term_cat\n",
    "    val c: Double = count_cat - count_term_cat\n",
    "    val d: Double = num_docs - count_cat - count_term + count_term_cat\n",
    "    \n",
    "    var chi_square: Double = (a*d) - (b*c)\n",
    "    chi_square = chi_square * chi_square\n",
    "    \n",
    "    chi_square = chi_square / (a+b)\n",
    "    chi_square = chi_square / (a+c)\n",
    "    chi_square = chi_square / (b+d)\n",
    "    chi_square = chi_square / (c+d)\n",
    "    \n",
    "    (category, (term, chi_square))\n",
    "})\n",
    "\n",
    "val result = chi_square_values.groupByKey().map(v => v._1.toString() + \" \" + v._2.toList.sortWith(_._2 > _._2).take(150).map(v => v._1+\":\"+v._2).mkString(\" \"))\n",
    "result.saveAsTextFile(\"output_rdd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f8db2cd-e960-43fa-affb-61ef70f8e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -getmerge hdfs:///user/e12132344/output_rdd output_rdd.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9266db5-9f27-435d-b186-b1f36fdadcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
